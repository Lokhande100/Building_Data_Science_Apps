{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building_Apps.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwRUs+xSZp/PS5eJMOrSa8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lokhande10/Building_Data_Science_Apps/blob/main/Building_Apps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install yfinance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NGbHt6Q45NN",
        "outputId": "0bf554fd-6127-4e65-865b-d63245b14b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Collecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 40 kB 29.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 61 kB 35.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 49.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.8.0 requests-2.27.1 yfinance-0.1.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6D-xslq45K5b",
        "outputId": "7ee4096e-1bc5-4ff6-d373-f48d0c0d303c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.5.1-py2.py3-none-any.whl (9.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 15.6 MB/s \n",
            "\u001b[?25hCollecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 43.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.4.0)\n",
            "Collecting blinker\n",
            "  Downloading blinker-1.4.tar.gz (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 60.9 MB/s \n",
            "\u001b[?25hCollecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Collecting validators\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.10.0.2)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (6.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.21.5)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.3.5)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.3)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (4.11.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (5.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=3.2.0->streamlit) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.1.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.5)\n",
            "Collecting ipykernel>=5.1.2\n",
            "  Downloading ipykernel-6.9.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 53.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Collecting ipython>=7.23.1\n",
            "  Downloading ipython-7.31.1-py3-none-any.whl (792 kB)\n",
            "\u001b[K     |████████████████████████████████| 792 kB 47.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.5.4)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.4.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.28-py3-none-any.whl (380 kB)\n",
            "\u001b[K     |████████████████████████████████| 380 kB 46.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (22.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.9.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.13.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.1.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (3.0.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Building wheels for collected packages: blinker\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=a3d420bb5e8584b0ff46f83cc8ee0efaea783358f077729a468693f583d30849\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
            "Successfully built blinker\n",
            "Installing collected packages: prompt-toolkit, ipython, ipykernel, smmap, gitdb, watchdog, validators, toml, pympler, pydeck, gitpython, blinker, base58, streamlit\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.9.1 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.31.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\u001b[0m\n",
            "Successfully installed base58-2.1.1 blinker-1.4 gitdb-4.0.9 gitpython-3.1.27 ipykernel-6.9.1 ipython-7.31.1 prompt-toolkit-3.0.28 pydeck-0.7.1 pympler-1.0.1 smmap-5.0.0 streamlit-1.5.1 toml-0.10.2 validators-0.18.2 watchdog-2.1.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import streamlit as st\n",
        "\n",
        "st.write(\"\"\"\n",
        "# Simple Stock Price App\n",
        "Shown are the stock closing price and volume of Google!\n",
        "\"\"\")\n",
        "\n",
        "# https://towardsdatascience.com/how-to-get-stock-data-using-python-c0de1df17e75\n",
        "#define the ticker symbol\n",
        "tickerSymbol = 'GOOGL'\n",
        "#get data on this ticker\n",
        "tickerData = yf.Ticker(tickerSymbol)\n",
        "#get the historical prices for this ticker\n",
        "tickerDf = tickerData.history(period='1d', start='2010-5-31', end='2020-5-31')\n",
        "# Open\tHigh\tLow\tClose\tVolume\tDividends\tStock Splits\n",
        "\n",
        "st.line_chart(tickerDf.Close)\n",
        "st.line_chart(tickerDf.Volume)"
      ],
      "metadata": {
        "id": "u-V-2aMy5ZGd",
        "outputId": "9cc5b498-20a1-4680-8329-07eff2955794",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-02-23 04:45:04.627 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Application 1   Stock price app"
      ],
      "metadata": {
        "id": "LE1UHtXv_Ngh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import streamlit as st\n",
        "\n",
        "st.write(\"\"\"\n",
        "# Simple Stock Price App\n",
        "Shown are the stock closing price and volume of Google!\n",
        "\"\"\")\n",
        "\n",
        "# https://towardsdatascience.com/how-to-get-stock-data-using-python-c0de1df17e75\n",
        "#define the ticker symbol\n",
        "tickerSymbol = 'GOOGL'\n",
        "#get data on this ticker\n",
        "tickerData = yf.Ticker(tickerSymbol)\n",
        "#get the historical prices for this ticker\n",
        "tickerDf = tickerData.history(period='1d', start='2010-5-31', end='2020-5-31')\n",
        "# Open\tHigh\tLow\tClose\tVolume\tDividends\tStock Splits\n",
        "\n",
        "st.line_chart(tickerDf.Close)\n",
        "st.line_chart(tickerDf.Volume)"
      ],
      "metadata": {
        "id": "jcjWQLDa_T7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Application_3 EDA NBA app"
      ],
      "metadata": {
        "id": "z8eDhxIp_Vra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mport streamlit as st\n",
        "import pandas as pd\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "st.title('NBA Player Stats Explorer')\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "This app performs simple webscraping of NBA player stats data!\n",
        "* **Python libraries:** base64, pandas, streamlit\n",
        "* **Data source:** [Basketball-reference.com](https://www.basketball-reference.com/).\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.header('User Input Features')\n",
        "selected_year = st.sidebar.selectbox('Year', list(reversed(range(1950,2020))))\n",
        "\n",
        "# Web scraping of NBA player stats\n",
        "@st.cache\n",
        "def load_data(year):\n",
        "    url = \"https://www.basketball-reference.com/leagues/NBA_\" + str(year) + \"_per_game.html\"\n",
        "    html = pd.read_html(url, header = 0)\n",
        "    df = html[0]\n",
        "    raw = df.drop(df[df.Age == 'Age'].index) # Deletes repeating headers in content\n",
        "    raw = raw.fillna(0)\n",
        "    playerstats = raw.drop(['Rk'], axis=1)\n",
        "    return playerstats\n",
        "playerstats = load_data(selected_year)\n",
        "\n",
        "# Sidebar - Team selection\n",
        "sorted_unique_team = sorted(playerstats.Tm.unique())\n",
        "selected_team = st.sidebar.multiselect('Team', sorted_unique_team, sorted_unique_team)\n",
        "\n",
        "# Sidebar - Position selection\n",
        "unique_pos = ['C','PF','SF','PG','SG']\n",
        "selected_pos = st.sidebar.multiselect('Position', unique_pos, unique_pos)\n",
        "\n",
        "# Filtering data\n",
        "df_selected_team = playerstats[(playerstats.Tm.isin(selected_team)) & (playerstats.Pos.isin(selected_pos))]\n",
        "\n",
        "st.header('Display Player Stats of Selected Team(s)')\n",
        "st.write('Data Dimension: ' + str(df_selected_team.shape[0]) + ' rows and ' + str(df_selected_team.shape[1]) + ' columns.')\n",
        "st.dataframe(df_selected_team)\n",
        "\n",
        "# Download NBA player stats data\n",
        "# https://discuss.streamlit.io/t/how-to-download-file-in-streamlit/1806\n",
        "def filedownload(df):\n",
        "    csv = df.to_csv(index=False)\n",
        "    b64 = base64.b64encode(csv.encode()).decode()  # strings <-> bytes conversions\n",
        "    href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"playerstats.csv\">Download CSV File</a>'\n",
        "    return href\n",
        "\n",
        "st.markdown(filedownload(df_selected_team), unsafe_allow_html=True)\n",
        "\n",
        "# Heatmap\n",
        "if st.button('Intercorrelation Heatmap'):\n",
        "    st.header('Intercorrelation Matrix Heatmap')\n",
        "    df_selected_team.to_csv('output.csv',index=False)\n",
        "    df = pd.read_csv('output.csv')\n",
        "\n",
        "    corr = df.corr()\n",
        "    mask = np.zeros_like(corr)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "    with sns.axes_style(\"white\"):\n",
        "        f, ax = plt.subplots(figsize=(7, 5))\n",
        "        ax = sns.heatmap(corr, mask=mask, vmax=1, square=True)\n",
        "    st.pyplot()"
      ],
      "metadata": {
        "id": "NpHRDYQ3GM9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#APP- 5 # S&P500 Data"
      ],
      "metadata": {
        "id": "AhrAIJSvhLi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "\n",
        "st.title('S&P 500 App')\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "This app retrieves the list of the **S&P 500** (from Wikipedia) and its corresponding **stock closing price** (year-to-date)!\n",
        "* **Python libraries:** base64, pandas, streamlit, numpy, matplotlib, seaborn\n",
        "* **Data source:** [Wikipedia](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies).\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.header('User Input Features')\n",
        "\n",
        "# Web scraping of S&P 500 data\n",
        "#\n",
        "@st.cache\n",
        "def load_data():\n",
        "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
        "    html = pd.read_html(url, header = 0)\n",
        "    df = html[0]\n",
        "    return df\n",
        "\n",
        "df = load_data()\n",
        "sector = df.groupby('GICS Sector')\n",
        "\n",
        "# Sidebar - Sector selection\n",
        "sorted_sector_unique = sorted( df['GICS Sector'].unique() )\n",
        "selected_sector = st.sidebar.multiselect('Sector', sorted_sector_unique, sorted_sector_unique)\n",
        "\n",
        "# Filtering data\n",
        "df_selected_sector = df[ (df['GICS Sector'].isin(selected_sector)) ]\n",
        "\n",
        "st.header('Display Companies in Selected Sector')\n",
        "st.write('Data Dimension: ' + str(df_selected_sector.shape[0]) + ' rows and ' + str(df_selected_sector.shape[1]) + ' columns.')\n",
        "st.dataframe(df_selected_sector)\n",
        "\n",
        "# Download S&P500 data\n",
        "# https://discuss.streamlit.io/t/how-to-download-file-in-streamlit/1806\n",
        "def filedownload(df):\n",
        "    csv = df.to_csv(index=False)\n",
        "    b64 = base64.b64encode(csv.encode()).decode()  # strings <-> bytes conversions\n",
        "    href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"SP500.csv\">Download CSV File</a>'\n",
        "    return href\n",
        "\n",
        "st.markdown(filedownload(df_selected_sector), unsafe_allow_html=True)\n",
        "\n",
        "# https://pypi.org/project/yfinance/\n",
        "\n",
        "data = yf.download(\n",
        "        tickers = list(df_selected_sector[:10].Symbol),\n",
        "        period = \"ytd\",\n",
        "        interval = \"1d\",\n",
        "        group_by = 'ticker',\n",
        "        auto_adjust = True,\n",
        "        prepost = True,\n",
        "        threads = True,\n",
        "        proxy = None\n",
        "    )\n",
        "\n",
        "# Plot Closing Price of Query Symbol\n",
        "def price_plot(symbol):\n",
        "  df = pd.DataFrame(data[symbol].Close)\n",
        "  df['Date'] = df.index\n",
        "  plt.fill_between(df.Date, df.Close, color='skyblue', alpha=0.3)\n",
        "  plt.plot(df.Date, df.Close, color='skyblue', alpha=0.8)\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.title(symbol, fontweight='bold')\n",
        "  plt.xlabel('Date', fontweight='bold')\n",
        "  plt.ylabel('Closing Price', fontweight='bold')\n",
        "  return st.pyplot()\n",
        "\n",
        "num_company = st.sidebar.slider('Number of Companies', 1, 5)\n",
        "\n",
        "if st.button('Show Plots'):\n",
        "    st.header('Stock Closing Price')\n",
        "    for i in list(df_selected_sector.Symbol)[:num_company]:\n",
        "        price_plot(i)"
      ],
      "metadata": {
        "id": "QPykEMQfhLlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#App_8"
      ],
      "metadata": {
        "id": "xA2xCb5NhLp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "st.write(\"\"\"\n",
        "# Penguin Prediction App\n",
        "This app predicts the **Palmer Penguin** species!\n",
        "Data obtained from the [palmerpenguins library](https://github.com/allisonhorst/palmerpenguins) in R by Allison Horst.\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.header('User Input Features')\n",
        "\n",
        "st.sidebar.markdown(\"\"\"\n",
        "[Example CSV input file](https://raw.githubusercontent.com/dataprofessor/data/master/penguins_example.csv)\n",
        "\"\"\")\n",
        "\n",
        "# Collects user input features into dataframe\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload your input CSV file\", type=[\"csv\"])\n",
        "if uploaded_file is not None:\n",
        "    input_df = pd.read_csv(uploaded_file)\n",
        "else:\n",
        "    def user_input_features():\n",
        "        island = st.sidebar.selectbox('Island',('Biscoe','Dream','Torgersen'))\n",
        "        sex = st.sidebar.selectbox('Sex',('male','female'))\n",
        "        bill_length_mm = st.sidebar.slider('Bill length (mm)', 32.1,59.6,43.9)\n",
        "        bill_depth_mm = st.sidebar.slider('Bill depth (mm)', 13.1,21.5,17.2)\n",
        "        flipper_length_mm = st.sidebar.slider('Flipper length (mm)', 172.0,231.0,201.0)\n",
        "        body_mass_g = st.sidebar.slider('Body mass (g)', 2700.0,6300.0,4207.0)\n",
        "        data = {'island': island,\n",
        "                'bill_length_mm': bill_length_mm,\n",
        "                'bill_depth_mm': bill_depth_mm,\n",
        "                'flipper_length_mm': flipper_length_mm,\n",
        "                'body_mass_g': body_mass_g,\n",
        "                'sex': sex}\n",
        "        features = pd.DataFrame(data, index=[0])\n",
        "        return features\n",
        "    input_df = user_input_features()\n",
        "\n",
        "# Combines user input features with entire penguins dataset\n",
        "# This will be useful for the encoding phase\n",
        "penguins_raw = pd.read_csv('penguins_cleaned.csv')\n",
        "penguins = penguins_raw.drop(columns=['species'])\n",
        "df = pd.concat([input_df,penguins],axis=0)\n",
        "\n",
        "# Encoding of ordinal features\n",
        "# https://www.kaggle.com/pratik1120/penguin-dataset-eda-classification-and-clustering\n",
        "encode = ['sex','island']\n",
        "for col in encode:\n",
        "    dummy = pd.get_dummies(df[col], prefix=col)\n",
        "    df = pd.concat([df,dummy], axis=1)\n",
        "    del df[col]\n",
        "df = df[:1] # Selects only the first row (the user input data)\n",
        "\n",
        "# Displays the user input features\n",
        "st.subheader('User Input features')\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    st.write(df)\n",
        "else:\n",
        "    st.write('Awaiting CSV file to be uploaded. Currently using example input parameters (shown below).')\n",
        "    st.write(df)\n",
        "\n",
        "# Reads in saved classification model\n",
        "load_clf = pickle.load(open('penguins_clf.pkl', 'rb'))\n",
        "\n",
        "# Apply model to make predictions\n",
        "prediction = load_clf.predict(df)\n",
        "prediction_proba = load_clf.predict_proba(df)\n",
        "\n",
        "\n",
        "st.subheader('Prediction')\n",
        "penguins_species = np.array(['Adelie','Chinstrap','Gentoo'])\n",
        "st.write(penguins_species[prediction])\n",
        "\n",
        "st.subheader('Prediction Probability')\n",
        "st.write(prediction_proba)"
      ],
      "metadata": {
        "id": "HfGX-jZT0_Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "penguins = pd.read_csv('penguins_cleaned.csv')\n",
        "\n",
        "# Ordinal feature encoding\n",
        "# https://www.kaggle.com/pratik1120/penguin-dataset-eda-classification-and-clustering\n",
        "df = penguins.copy()\n",
        "target = 'species'\n",
        "encode = ['sex','island']\n",
        "\n",
        "for col in encode:\n",
        "    dummy = pd.get_dummies(df[col], prefix=col)\n",
        "    df = pd.concat([df,dummy], axis=1)\n",
        "    del df[col]\n",
        "\n",
        "target_mapper = {'Adelie':0, 'Chinstrap':1, 'Gentoo':2}\n",
        "def target_encode(val):\n",
        "    return target_mapper[val]\n",
        "\n",
        "df['species'] = df['species'].apply(target_encode)\n",
        "\n",
        "# Separating X and y\n",
        "X = df.drop('species', axis=1)\n",
        "Y = df['species']\n",
        "\n",
        "# Build random forest model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X, Y)\n",
        "\n",
        "# Saving the model\n",
        "import pickle\n",
        "pickle.dump(clf, open('penguins_clf.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "kQQfIUBJ1JPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#App Boston housing"
      ],
      "metadata": {
        "id": "svGkoh4z2vhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "st.write(\"\"\"\n",
        "# Boston House Price Prediction App\n",
        "This app predicts the **Boston House Price**!\n",
        "\"\"\")\n",
        "st.write('---')\n",
        "\n",
        "# Loads the Boston House Price Dataset\n",
        "boston = datasets.load_boston()\n",
        "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "Y = pd.DataFrame(boston.target, columns=[\"MEDV\"])\n",
        "\n",
        "# Sidebar\n",
        "# Header of Specify Input Parameters\n",
        "st.sidebar.header('Specify Input Parameters')\n",
        "\n",
        "def user_input_features():\n",
        "    CRIM = st.sidebar.slider('CRIM', X.CRIM.min(), X.CRIM.max(), X.CRIM.mean())\n",
        "    ZN = st.sidebar.slider('ZN', X.ZN.min(), X.ZN.max(), X.ZN.mean())\n",
        "    INDUS = st.sidebar.slider('INDUS', X.INDUS.min(), X.INDUS.max(), X.INDUS.mean())\n",
        "    CHAS = st.sidebar.slider('CHAS', X.CHAS.min(), X.CHAS.max(), X.CHAS.mean())\n",
        "    NOX = st.sidebar.slider('NOX', X.NOX.min(), X.NOX.max(), X.NOX.mean())\n",
        "    RM = st.sidebar.slider('RM', X.RM.min(), X.RM.max(), X.RM.mean())\n",
        "    AGE = st.sidebar.slider('AGE', X.AGE.min(), X.AGE.max(), X.AGE.mean())\n",
        "    DIS = st.sidebar.slider('DIS', X.DIS.min(), X.DIS.max(), X.DIS.mean())\n",
        "    RAD = st.sidebar.slider('RAD', X.RAD.min(), X.RAD.max(), X.RAD.mean())\n",
        "    TAX = st.sidebar.slider('TAX', X.TAX.min(), X.TAX.max(), X.TAX.mean())\n",
        "    PTRATIO = st.sidebar.slider('PTRATIO', X.PTRATIO.min(), X.PTRATIO.max(), X.PTRATIO.mean())\n",
        "    B = st.sidebar.slider('B', X.B.min(), X.B.max(), X.B.mean())\n",
        "    LSTAT = st.sidebar.slider('LSTAT', X.LSTAT.min(), X.LSTAT.max(), X.LSTAT.mean())\n",
        "    data = {'CRIM': CRIM,\n",
        "            'ZN': ZN,\n",
        "            'INDUS': INDUS,\n",
        "            'CHAS': CHAS,\n",
        "            'NOX': NOX,\n",
        "            'RM': RM,\n",
        "            'AGE': AGE,\n",
        "            'DIS': DIS,\n",
        "            'RAD': RAD,\n",
        "            'TAX': TAX,\n",
        "            'PTRATIO': PTRATIO,\n",
        "            'B': B,\n",
        "            'LSTAT': LSTAT}\n",
        "    features = pd.DataFrame(data, index=[0])\n",
        "    return features\n",
        "\n",
        "df = user_input_features()\n",
        "\n",
        "# Main Panel\n",
        "\n",
        "# Print specified input parameters\n",
        "st.header('Specified Input parameters')\n",
        "st.write(df)\n",
        "st.write('---')\n",
        "\n",
        "# Build Regression Model\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X, Y)\n",
        "# Apply Model to Make Prediction\n",
        "prediction = model.predict(df)\n",
        "\n",
        "st.header('Prediction of MEDV')\n",
        "st.write(prediction)\n",
        "st.write('---')\n",
        "\n",
        "# Explaining the model's predictions using SHAP values\n",
        "# https://github.com/slundberg/shap\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X)\n",
        "\n",
        "st.header('Feature Importance')\n",
        "plt.title('Feature importance based on SHAP values')\n",
        "shap.summary_plot(shap_values, X)\n",
        "st.pyplot(bbox_inches='tight')\n",
        "st.write('---')\n",
        "\n",
        "plt.title('Feature importance based on SHAP values (Bar)')\n",
        "shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
        "st.pyplot(bbox_inches='tight')"
      ],
      "metadata": {
        "id": "dUgm8Cyr2zmt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}